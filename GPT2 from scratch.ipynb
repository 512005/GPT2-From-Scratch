{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import GPT2Tokenizer\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfrom tqdm import tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T17:58:37.073492Z","iopub.execute_input":"2025-05-26T17:58:37.073963Z","iopub.status.idle":"2025-05-26T17:58:37.077901Z","shell.execute_reply.started":"2025-05-26T17:58:37.073942Z","shell.execute_reply":"2025-05-26T17:58:37.077174Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"33333333333333333333333333333333333333333333333333","metadata":{}},{"cell_type":"code","source":"# Cell 1: Imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import GPT2Tokenizer\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom datasets import load_dataset\n\n# Cell 2: Positional Encoding\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)  # [max_len, d_model]\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # [max_len, 1]\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)  # Shape: [1, max_len, d_model]\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        # x shape: [batch_size, seq_len, d_model]\n        return x + self.pe[:, :x.size(1), :]\n\n# Cell 3: MultiHeadAttention\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        assert d_model % num_heads == 0\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n        self.scale = math.sqrt(self.d_k)\n\n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n        if mask is not None:\n            # mask shape expected: [batch_size, 1, seq_len, seq_len] or broadcastable\n            scores = scores.masked_fill(mask == 0, float('-inf'))\n        attn = torch.softmax(scores, dim=-1)\n        output = torch.matmul(attn, V)\n        return output, attn\n\n    def forward(self, x, mask=None):\n        batch_size, seq_len, _ = x.size()\n        Q = self.W_q(x)\n        K = self.W_k(x)\n        V = self.W_v(x)\n\n        # Reshape for multi-head attention\n        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)  # [b, h, seq_len, d_k]\n        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n\n        context, attn = self.scaled_dot_product_attention(Q, K, V, mask)\n        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n        return self.W_o(context), attn\n\n# Cell 4: FeedForward\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        return self.linear2(self.relu(self.linear1(x)))\n\n# Cell 5: TransformerBlock\nclass TransformerBlock(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n        self.attn = MultiHeadAttention(d_model, num_heads)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.ff = FeedForward(d_model, d_ff)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        attn_output, attn_weights = self.attn(x, mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        ff_output = self.ff(x)\n        x = self.norm2(x + self.dropout(ff_output))\n        return x, attn_weights\n\n# Cell 6: GPT2 Model\nclass GPT2(nn.Module):\n    def __init__(self, vocab_size, d_model=128, num_heads=4, num_layers=2, d_ff=512, max_len=512, dropout=0.1):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.pos_encoding = PositionalEncoding(d_model, max_len)\n        self.transformer_blocks = nn.ModuleList([\n            TransformerBlock(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n        ])\n        self.output_layer = nn.Linear(d_model, vocab_size)\n        self.dropout = nn.Dropout(dropout)\n        self.max_len = max_len\n\n    def forward(self, x, mask=None):\n        batch_size, seq_len = x.size()\n        x = self.embedding(x)  # [b, seq_len, d_model]\n        x = self.pos_encoding(x)\n        x = self.dropout(x)\n\n        attn_weights = []\n\n        # Prepare masks for causal attention\n        # causal mask: (seq_len, seq_len), lower triangular matrix (1's for allowed positions)\n        causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=x.device)).bool()\n\n        if mask is not None:\n            # mask: [batch_size, seq_len] (1 for tokens, 0 for padding)\n            # Expand mask shape for broadcasting: [batch_size, 1, 1, seq_len]\n            mask = mask.unsqueeze(1).unsqueeze(2)\n            # Expand causal mask for batch and heads: [1, 1, seq_len, seq_len]\n            causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)\n            combined_mask = mask & causal_mask  # broadcasted AND\n        else:\n            # Just causal mask for all tokens\n            combined_mask = causal_mask.unsqueeze(0).unsqueeze(0)  # shape [1,1,seq_len,seq_len]\n\n        for block in self.transformer_blocks:\n            x, attn = block(x, combined_mask)\n            attn_weights.append(attn)\n\n        logits = self.output_layer(x)  # [batch_size, seq_len, vocab_size]\n        return logits, attn_weights\n\n# Cell 7: Prepare Dataset and DataLoaders\n# Load TinyStories dataset from HuggingFace datasets\ndataset = load_dataset(\"roneneldan/TinyStories\", split=\"train[:1%]\")\nval_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"validation[:1%]\")\n\n# Initialize tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n\n# Tokenize dataset\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\ntokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n\ntokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\ntokenized_val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n\n# Define PyTorch Dataset wrapper\nclass TinyStoriesDataset(Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __len__(self):\n        return len(self.encodings[\"input_ids\"])\n\n    def __getitem__(self, idx):\n        input_ids = self.encodings[\"input_ids\"][idx]\n        attention_mask = self.encodings[\"attention_mask\"][idx]\n        return input_ids, attention_mask\n\ntrain_dataset = TinyStoriesDataset(tokenized_dataset)\nval_dataset = TinyStoriesDataset(tokenized_val_dataset)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=8)\n\n# Cell 8: Training function\ndef train_model(model, train_loader, val_loader, tokenizer, num_epochs=5, device='cuda'):\n    optimizer = optim.Adam(model.parameters(), lr=3e-4)\n    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n\n    train_losses = []\n    val_losses = []\n\n    model.to(device)\n\n    for epoch in range(num_epochs):\n        model.train()\n        total_train_loss = 0\n        for xb, mask in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Training]\"):\n            xb, mask = xb.to(device), mask.to(device)\n            logits, _ = model(xb, mask)\n\n            # Shift logits and labels for next token prediction\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = xb[..., 1:].contiguous()\n\n            loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)),\n                           shift_labels.view(-1))\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_train_loss += loss.item()\n\n        avg_train_loss = total_train_loss / len(train_loader)\n        train_losses.append(avg_train_loss)\n\n        model.eval()\n        total_val_loss = 0\n        with torch.no_grad():\n            for xb, mask in val_loader:\n                xb, mask = xb.to(device), mask.to(device)\n                logits, _ = model(xb, mask)\n\n                shift_logits = logits[..., :-1, :].contiguous()\n                shift_labels = xb[..., 1:].contiguous()\n\n                loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)),\n                               shift_labels.view(-1))\n                total_val_loss += loss.item()\n\n        avg_val_loss = total_val_loss / len(val_loader)\n        val_losses.append(avg_val_loss)\n\n        print(f\"Epoch {epoch+1} Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n\n    # Plot losses\n    plt.plot(train_losses, label=\"Train Loss\")\n    plt.plot(val_losses, label=\"Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()\n\n# Cell 9: Run training\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nvocab_size = tokenizer.vocab_size\nmodel = GPT2(vocab_size=vocab_size)\n\ntrain_model(model, train_loader, val_loader, tokenizer, num_epochs=5, device=device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:07:51.098758Z","iopub.execute_input":"2025-05-26T18:07:51.099045Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/220 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"546bf0e779f54300a956cb7814c10ab1"}},"metadata":{}},{"name":"stderr","text":"Epoch 1 [Training]: 100%|██████████| 2650/2650 [17:07<00:00,  2.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Train Loss: 4.1866 | Val Loss: 3.1371\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 [Training]: 100%|██████████| 2650/2650 [16:59<00:00,  2.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Train Loss: 3.3443 | Val Loss: 2.8189\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 [Training]: 100%|██████████| 2650/2650 [16:57<00:00,  2.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Train Loss: 3.1114 | Val Loss: 2.6523\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 [Training]: 100%|██████████| 2650/2650 [17:00<00:00,  2.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 Train Loss: 2.9714 | Val Loss: 2.5459\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5 [Training]:  48%|████▊     | 1283/2650 [08:15<08:44,  2.60it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"def generate_story(model, tokenizer, prompt=\"once upon a time\", max_length=100, device='cpu'):\n    model.eval()\n    tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)  # shape: [1, seq_len]\n    generated = tokens\n\n    with torch.no_grad():\n        for _ in range(max_length):\n            logits, _ = model(generated)\n            next_token_logits = logits[:, -1, :]  # last token logits\n            next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)  # greedy\n            generated = torch.cat((generated, next_token_id), dim=1)\n            if next_token_id.item() == tokenizer.eos_token_id:\n                break\n\n    story = tokenizer.decode(generated[0], skip_special_tokens=True)\n    return story\n\n# Usage after training:\nmodel.to(device)\nprompt = \"once upon a time\"\nprint(generate_story(model, tokenizer, prompt, max_length=100, device=device))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}