{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11913777,"sourceType":"datasetVersion","datasetId":7490010},{"sourceId":11938061,"sourceType":"datasetVersion","datasetId":7505444}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\n# List all files in the input directory\nfor root, dirs, files in os.walk(\"/kaggle/input\"):\n    print(\"üìÅ\", root)\n    for file in files:\n        print(\"   üìÑ\", file)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T21:27:18.218938Z","iopub.execute_input":"2025-05-24T21:27:18.219312Z","iopub.status.idle":"2025-05-24T21:27:18.231922Z","shell.execute_reply.started":"2025-05-24T21:27:18.219283Z","shell.execute_reply":"2025-05-24T21:27:18.231037Z"}},"outputs":[{"name":"stdout","text":"üìÅ /kaggle/input\nüìÅ /kaggle/input/gpt2-tokenizer\n   üìÑ merges.txt\n   üìÑ vocab.json\n   üìÑ tokenizer_config.json\nüìÅ /kaggle/input/tinystories\n   üìÑ TinyStories-validation-small.txt\n   üìÑ TinyStories-valid.txt\n   üìÑ TinyStories-train-small.txt\n   üìÑ TinyStories-train.txt\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Cell 1: Imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import GPT2Tokenizer\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfrom tqdm import tqdm","metadata":{"_uuid":"35ce3653-5c58-4963-8594-ba4aac50a8da","_cell_guid":"45b3e7ec-b3e3-4d35-8f52-83a165a89534","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-26T14:44:50.503779Z","iopub.execute_input":"2025-05-26T14:44:50.504690Z","iopub.status.idle":"2025-05-26T14:44:50.511265Z","shell.execute_reply.started":"2025-05-26T14:44:50.504653Z","shell.execute_reply":"2025-05-26T14:44:50.510067Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Cell 2: Positional Encoding\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)  # Shape: [1, max_len, d_model]\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1), :]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T14:45:00.044669Z","iopub.execute_input":"2025-05-26T14:45:00.045031Z","iopub.status.idle":"2025-05-26T14:45:00.052951Z","shell.execute_reply.started":"2025-05-26T14:45:00.044982Z","shell.execute_reply":"2025-05-26T14:45:00.051837Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        assert d_model % num_heads == 0\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n        \n        self.scale = math.sqrt(self.d_k)\n\n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        # Q, K, V shape: [batch_size, num_heads, seq_len, d_k]\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale  # [B, H, T, T]\n\n        if mask is not None:\n            # mask shape expected: [batch_size, seq_len, seq_len]\n            if mask.dim() == 3:\n                mask = mask.unsqueeze(1)  # [B, 1, T, T], to broadcast over heads\n            scores = scores.masked_fill(mask == 0, float('-inf'))\n\n        attn = torch.softmax(scores, dim=-1)  # attention weights\n        output = torch.matmul(attn, V)         # weighted sum\n        return output, attn\n\n    def forward(self, x, mask=None):\n        batch_size, seq_len, _ = x.size()\n\n        Q = self.W_q(x)\n        K = self.W_k(x)\n        V = self.W_v(x)\n\n        # reshape for multi-head\n        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n\n        context, attn = self.scaled_dot_product_attention(Q, K, V, mask)\n        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n        return self.W_o(context), attn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T14:57:17.039171Z","iopub.execute_input":"2025-05-26T14:57:17.039498Z","iopub.status.idle":"2025-05-26T14:57:17.050605Z","shell.execute_reply.started":"2025-05-26T14:57:17.039479Z","shell.execute_reply":"2025-05-26T14:57:17.049300Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"# Cell 4: FeedForward\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        return self.linear2(self.relu(self.linear1(x)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T14:45:27.239671Z","iopub.execute_input":"2025-05-26T14:45:27.239950Z","iopub.status.idle":"2025-05-26T14:45:27.245961Z","shell.execute_reply.started":"2025-05-26T14:45:27.239931Z","shell.execute_reply":"2025-05-26T14:45:27.244916Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"# Cell 5: TransformerBlock\nclass TransformerBlock(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n        self.attn = MultiHeadAttention(d_model, num_heads)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.ff = FeedForward(d_model, d_ff)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        attn_output, attn_weights = self.attn(x, mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        ff_output = self.ff(x)\n        x = self.norm2(x + self.dropout(ff_output))\n        return x, attn_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T14:45:38.241104Z","iopub.execute_input":"2025-05-26T14:45:38.242270Z","iopub.status.idle":"2025-05-26T14:45:38.249357Z","shell.execute_reply.started":"2025-05-26T14:45:38.242235Z","shell.execute_reply":"2025-05-26T14:45:38.248094Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"class GPT2(nn.Module):\n    def __init__(self, vocab_size, d_model=128, num_heads=4, num_layers=2, d_ff=512, max_len=512, dropout=0.1):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.pos_encoding = PositionalEncoding(d_model, max_len)\n        self.transformer_blocks = nn.ModuleList([\n            TransformerBlock(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n        ])\n        self.output_layer = nn.Linear(d_model, vocab_size)\n        self.dropout = nn.Dropout(dropout)\n        self.max_len = max_len\n\n    def forward(self, x, mask=None):\n        batch_size, seq_len = x.size()\n        \n        x = self.embedding(x)\n        x = self.pos_encoding(x)\n        x = self.dropout(x)\n\n        attn_weights = []\n        if mask is not None:\n            # Dataset mask is [batch_size, seq_len], convert to [batch_size, 1, seq_len, seq_len]\n            mask = mask.unsqueeze(-1)  # [batch_size, seq_len, 1]\n            mask = mask.unsqueeze(1)   # [batch_size, 1, seq_len, 1]\n            # Create a causal mask\n            causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1).bool()\n            causal_mask = ~causal_mask  # Lower triangle (including diagonal) is True\n            # Combine dataset mask with causal mask\n            mask = mask * causal_mask.unsqueeze(0)  # [batch_size, 1, seq_len, seq_len]\n        \n        for block in self.transformer_blocks:\n            x, attn = block(x, mask)\n            attn_weights.append(attn)\n\n        logits = self.output_layer(x)\n        return logits, attn_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T14:55:36.449244Z","iopub.execute_input":"2025-05-26T14:55:36.450149Z","iopub.status.idle":"2025-05-26T14:55:36.460101Z","shell.execute_reply.started":"2025-05-26T14:55:36.450115Z","shell.execute_reply":"2025-05-26T14:55:36.458861Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# Cell 7: Dataset\nclass TinyStoriesDataset(Dataset):\n    def __init__(self, data_path, max_len=512):\n        self.tokenizer = GPT2Tokenizer.from_pretrained(\"/kaggle/input/gpt2-tokenizer\")\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.max_len = max_len\n        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n            self.lines = f.readlines()\n\n    def __len__(self):\n        return len(self.lines)\n\n    def __getitem__(self, idx):\n        line = self.lines[idx]\n        encoding = self.tokenizer(\n            line,\n            truncation=True,\n            max_length=self.max_len,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n        input_ids = encoding[\"input_ids\"].squeeze()\n        attention_mask = encoding[\"attention_mask\"].squeeze()\n        return input_ids, attention_mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T14:46:01.481472Z","iopub.execute_input":"2025-05-26T14:46:01.481827Z","iopub.status.idle":"2025-05-26T14:46:01.489933Z","shell.execute_reply.started":"2025-05-26T14:46:01.481802Z","shell.execute_reply":"2025-05-26T14:46:01.488815Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# Cell 8: Initialize tokenizer and datasets\ntokenizer = GPT2Tokenizer.from_pretrained(\"/kaggle/input/gpt2-tokenizer\")\ntokenizer.pad_token = tokenizer.eos_token\n\ntrain_dataset = TinyStoriesDataset(\"/kaggle/input/tinystories/TinyStories-train-small.txt\")\nval_dataset = TinyStoriesDataset(\"/kaggle/input/tinystories/TinyStories-validation-small.txt\")\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=8)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T14:46:15.367966Z","iopub.execute_input":"2025-05-26T14:46:15.368914Z","iopub.status.idle":"2025-05-26T14:46:17.334676Z","shell.execute_reply.started":"2025-05-26T14:46:15.368887Z","shell.execute_reply":"2025-05-26T14:46:17.333691Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# Cell 9: Training Function\ndef train_model(model, train_loader, val_loader, tokenizer, num_epochs=5, device='cuda'):\n    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n\n    train_losses = []\n    val_losses = []\n\n    model.to(device)\n\n    for epoch in range(num_epochs):\n        model.train()\n        total_train_loss = 0\n        for xb, mask in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Training]\"):\n            xb, mask = xb.to(device), mask.to(device)\n            logits, _ = model(xb, mask)\n            \n            # Shift input and target for language modeling\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = xb[..., 1:].contiguous()\n            \n            loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), \n                          shift_labels.view(-1))\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_train_loss += loss.item()\n        \n        avg_train_loss = total_train_loss / len(train_loader)\n        train_losses.append(avg_train_loss)\n\n        # Validation\n        model.eval()\n        total_val_loss = 0\n        with torch.no_grad():\n            for xb, mask in val_loader:\n                xb, mask = xb.to(device), mask.to(device)\n                logits, _ = model(xb, mask)\n                \n                shift_logits = logits[..., :-1, :].contiguous()\n                shift_labels = xb[..., 1:].contiguous()\n                \n                loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), \n                              shift_labels.view(-1))\n                total_val_loss += loss.item()\n        \n        avg_val_loss = total_val_loss / len(val_loader)\n        val_losses.append(avg_val_loss)\n\n        print(f\"‚úÖ Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n    \n    return train_losses, val_losses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T14:46:31.520420Z","iopub.execute_input":"2025-05-26T14:46:31.520756Z","iopub.status.idle":"2025-05-26T14:46:31.534274Z","shell.execute_reply.started":"2025-05-26T14:46:31.520733Z","shell.execute_reply":"2025-05-26T14:46:31.533081Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# Cell 10: Initialize and train model\nmodel = GPT2(vocab_size=tokenizer.vocab_size, d_model=128, num_heads=4, num_layers=2, d_ff=512)\nmodel.to(device)\ntrain_losses, val_losses = train_model(model, train_loader, val_loader, tokenizer, num_epochs=5, device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T14:57:23.269118Z","iopub.execute_input":"2025-05-26T14:57:23.269834Z"}},"outputs":[{"name":"stderr","text":"Epoch 1 [Training]:   1%|          | 2910/277791 [5:17:06<505:57:14,  6.63s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Cell 11: Evaluation and Generation Functions\ndef compute_perplexity(model, data_loader, device='cuda'):\n    model.eval()\n    total_loss = 0\n    total_tokens = 0\n    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, reduction='sum')\n\n    with torch.no_grad():\n        for input_ids, mask in data_loader:\n            input_ids, mask = input_ids.to(device), mask.to(device)\n            logits, _ = model(input_ids, mask)\n            \n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = input_ids[..., 1:].contiguous()\n            \n            loss = criterion(shift_logits.view(-1, shift_logits.size(-1)), \n                            shift_labels.view(-1))\n            total_loss += loss.item()\n            total_tokens += (shift_labels != tokenizer.pad_token_id).sum().item()\n\n    avg_loss = total_loss / total_tokens\n    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n    return perplexity\n\ndef top_k_filtering(logits, top_k):\n    top_k = min(top_k, logits.size(-1))\n    indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n    logits[indices_to_remove] = -float('Inf')\n    return logits\n\ndef generate_text(model, tokenizer, prompt, max_len=100, temperature=0.7, top_k=50, device='cuda'):\n    model.eval()\n    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n    generated = input_ids\n    \n    with torch.no_grad():\n        for _ in range(max_len):\n            logits, _ = model(generated)\n            next_token_logits = logits[:, -1, :] / temperature\n            filtered_logits = top_k_filtering(next_token_logits, top_k)\n            next_token = torch.multinomial(torch.softmax(filtered_logits, dim=-1), num_samples=1)\n            generated = torch.cat((generated, next_token), dim=1)\n            if next_token.item() == tokenizer.eos_token_id:\n                break\n    \n    return tokenizer.decode(generated[0], skip_special_tokens=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 12: Evaluate and Generate\nperplexity = compute_perplexity(model, val_loader, device=device)\nprint(f\"Perplexity: {perplexity:.4f}\")\n\nprompt = \"Once upon a time\"\ngenerated_text = generate_text(model, tokenizer, prompt, max_len=100, device=device)\nprint(\"Generated Text:\", generated_text)\n\n# Plot training curves\nplt.figure(figsize=(8, 6))\nplt.plot(range(1, len(train_losses)+1), train_losses, label='Training Loss')\nplt.plot(range(1, len(val_losses)+1), val_losses, label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T14:32:09.419648Z","iopub.execute_input":"2025-05-26T14:32:09.419982Z","iopub.status.idle":"2025-05-26T14:32:09.428957Z","shell.execute_reply.started":"2025-05-26T14:32:09.419959Z","shell.execute_reply":"2025-05-26T14:32:09.427792Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"}],"execution_count":15}]}